{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1.2, 2.1, 5.9]\n",
    "weights = [3.1, 2.3, 3.5]\n",
    "bias = 3\n",
    "\n",
    "output = inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.2\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# what if we have multiple weights and bias\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, .26, -.5],\n",
    "           [-.26, -.27, .17, .87]]\n",
    "bias = [2, 3, .5]\n",
    "\n",
    "layer_output = []\n",
    "for neuron_weights, neuron_bias in zip(weights, bias):\n",
    "    neuron_output = 0\n",
    "    for n_input, weights in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input*weights\n",
    "    neuron_output += neuron_bias\n",
    "    layer_output.append(neuron_output)\n",
    "    \n",
    "print(layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding shape before moving further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l = [1,2,3,4]\n",
    "        - shape = (4,)\n",
    "        - type = 1D Array, Vector\n",
    "        \n",
    "l = [[1,2,3,4],\n",
    "     [2,3,4,5]]\n",
    "         - shape = (2,4)\n",
    "         - type = 2D Array, Metrix\n",
    "        \n",
    "l = [  [ [1,2,3,4],\n",
    "         [9,8,7,6] ]\n",
    "       [ [2,3,4,5],\n",
    "         [8,7,6,5] ]\n",
    "       [ [3,4,5,6],\n",
    "         [7,6,5,4] ]  ]\n",
    "         - shape = (3,2,4)\n",
    "         - type = 3D Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Dot Product\n",
    "import numpy as np\n",
    "inputs = [1.2, 2.1, 5.9]\n",
    "weights = [3.1, 2.3, 3.5]\n",
    "bias = 3\n",
    "\n",
    "output = np.dot(weights,inputs) + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8  , 1.21 , 2.385])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, .26, -.5],\n",
    "           [-.26, -.27, .17, .87]]\n",
    "bias = [2, 3, .5]\n",
    "\n",
    "output = np.dot(weights,inputs) + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "- The batchs are needed because if we provide one point at a time our model got disturbe and start behaving madly.\n",
    "- Also if we provide all the values for training our model will over fit to all the values.\n",
    "- To handel this batches are to be made so that our model work finly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4b3ccc34e1bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2, 5, 3, 2.5],\n",
    "          [-1.5, 2.7, 3.3, -.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, .26, -.5],\n",
    "           [-.26, -.27, .17, .87]]\n",
    "\n",
    "bias = [2, 3, .5]\n",
    "\n",
    "output = np.dot(weights,inputs) + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above portion gives error because when ever we multiply 2 metrices the row muptiply with columns\n",
    "         Like first row with first columns and so on.\n",
    "         row[3,4] and columns[3,4]\n",
    "- But in our case we have 3 rows and 4 columns that's why it is throwing error.\n",
    "- To handel this we first have to Transpose our weights.\n",
    "         This will make \n",
    "                row[3,4] and columns[4,3]\n",
    "                \n",
    "- Row(Input) and Column(Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.8  ,  1.21 ,  2.385],\n",
       "       [ 7.4  , -1.02 ,  1.315],\n",
       "       [ 1.41 ,  1.051,  0.026]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2, 5, 3, 2.5],\n",
    "          [-1.5, 2.7, 3.3, -.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, .26, -.5],\n",
    "           [-.26, -.27, .17, .87]]\n",
    "\n",
    "bias = [2, 3, .5]\n",
    "\n",
    "output = np.dot(inputs, np.array(weights).T) + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another layer of weight and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.2965  -6.8426   1.33815]\n",
      " [ 3.5285  -9.7516  -1.73825]\n",
      " [ 2.6379  -2.99426  0.83199]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2, 5, -1, 2.5],\n",
    "          [-1.5, 2.7, 3.3, -.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, .26, -.5],\n",
    "           [-.26, -.27, .17, .87]]\n",
    "bias = [2, 3, .5]\n",
    "\n",
    "weights2 = [[0.5, 0.9, -0.5],\n",
    "           [-0.5, 0.7, -.96],\n",
    "           [-.26, .27, .57]]\n",
    "bias2 = [1, -3, .9]\n",
    "\n",
    "layer1_output = np.dot(inputs, np.array(weights).T) + bias\n",
    "\n",
    "# now the output of layer one will the input for layer 2\n",
    "layer2_output = np.dot(layer1_output, np.array(weights2).T) + bias2\n",
    "\n",
    "print(layer2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always have to keep in our mind that our data lie b/w -1 to 1 so that out data wont explode.\n",
    "This is just an example thats why I have taken some hiher values.\n",
    "But whenever we work with real data we have to first normalize our data so that we get good output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14629144 -0.04806473 -0.00102545]\n",
      " [ 0.08093017 -0.17912034 -0.04057348]\n",
      " [ 0.09165886 -0.05176894 -0.09859882]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = [[1, 2, 3, 2.5],                           #input feature set\n",
    "     [2, 5, -1, 2.5],\n",
    "     [-1.5, 2.7, 3.3, -.8]]\n",
    "\n",
    "class layers:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)    # converting random values from range -1 to 1\n",
    "        self.bias = np.zeros((1, n_neurons))\n",
    "    def forword(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "\n",
    "layer1 = layers(4,6)\n",
    "layer2 = layers(6,3)\n",
    "\n",
    "layer1.forword(X)\n",
    "#print(layer1.output)\n",
    "\n",
    "layer2.forword(layer1.output)\n",
    "print(layer2.output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "- Sigmoid\n",
    "- Step function\n",
    "- ReLU (Most commonly used)(Fast){Rectified Linear Unit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.06681425  0.78316807 -0.00732435  0.46266864  0.32090059]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "\n",
      "{Basically Activation function here converts negative values to 0}\n",
      "\n",
      "[[0.10758131 1.03983522 0.24462411 0.31821498 0.18851053]\n",
      " [0.         0.78316807 0.         0.46266864 0.32090059]\n",
      " [0.         0.55688422 0.07987797 0.         0.04553042]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = [[1, 2, 3, 2.5],                           #input feature set\n",
    "     [2, 5, -1, 2.5],\n",
    "     [-1.5, 2.7, 3.3, -.8]]\n",
    "\n",
    "class layers:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)    # converting random values from range -1 to 1\n",
    "        self.bias = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "layer1 = layers(4,5)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "\n",
    "print(layer1.output)\n",
    "print()\n",
    "print(\"{Basically Activation function here converts negative values to 0}\")\n",
    "print()\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
